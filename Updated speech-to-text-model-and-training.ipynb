{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config: \n",
    "    train_batch_size=8 \n",
    "    val_batch_size=32\n",
    "    num_res_cnn=3 \n",
    "    num_rnn=5 \n",
    "    input_features=128 \n",
    "\n",
    "    #residual_cnn_params\n",
    "    res_cnn_input_channels=32 \n",
    "    res_cnn_output_channels=32 \n",
    "    res_cnn_kernel_size=3 \n",
    "    res_cnn_stride=1    \n",
    "    res_cnn_padding=1 \n",
    "    res_cnn_dropout_probability=0.1 \n",
    "    \n",
    "    #bi directional rnn params\n",
    "    bi_rnn_input_size=512 \n",
    "    bi_rnn_hidden_size=512 \n",
    "    bi_rnn_num_layers=1 \n",
    "    bi_rnn_dropout_probability=0.1 \n",
    "    \n",
    "    #final classifier block params\n",
    "    cl_input_size=bi_rnn_input_size*2 \n",
    "    cl_dropout_probability=0.1 \n",
    "    cl_num_classes=29 \n",
    "    \n",
    "#     optimizatson\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    lr=1e-5\n",
    "    epochs=2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data LIBRISPEECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "train_data=torchaudio.datasets.LIBRISPEECH('./',url='dev-clean',download=True) \n",
    "val_data=torchaudio.datasets.LIBRISPEECH('./',url='test-clean',download=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can also use custom Data set. \n",
    "## Load custom Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class LibriSpeechFlacDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.audio_paths = list(Path(root_dir).rglob(\"*.flac\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform, sample_rate, str(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "train_data = LibriSpeechFlacDataset('./LibriSpeech/dev-clean')\n",
    "val_data = LibriSpeechFlacDataset('./LibriSpeech/test-clean')\n",
    "\n",
    "all_data = ConcatDataset([train_data, val_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms=nn.Sequential(\n",
    "                                torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=Config.input_features),\n",
    "                                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "                                torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    "                                )\n",
    "# after the transformation what we get is (batch ,channel,feature,time)\n",
    "val_transforms=nn.Sequential(\n",
    "                                torchaudio.transforms.MelSpectrogram()\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_data=itertools.chain(train_data,val_data)\n",
    "tensor_max_length=-1\n",
    "label_max_length=-1\n",
    "for instance in all_data:\n",
    "    data_len=train_transforms(instance[0]).shape[-1]    \n",
    "    label_len=len(instance[2])\n",
    "    \n",
    "    if data_len>tensor_max_length:\n",
    "        tensor_max_length=data_len\n",
    "    if label_len>label_max_length:\n",
    "        label_max_length=label_len\n",
    "        \n",
    "    \n",
    "tensor_max_length,label_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=[\"'\",' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "nums=range(28)\n",
    "alpha_to_num_dict={alphas[i]:nums[i] for i in range(len(alphas))}\n",
    "num_to_alpha_dict={v:k for k,v in alpha_to_num_dict.items()}\n",
    "\n",
    "\n",
    "def num_to_alpha(number:torch.tensor):\n",
    "    return num_to_alpha_dict[number.item()] \n",
    "\n",
    "def alpha_to_num(alpha):\n",
    "    return alpha_to_num_dict[alpha.lower()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self,data,transforms):\n",
    "        self.data=data\n",
    "        self.transforms=transforms\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        single_audio,transcript=self.transforms(self.data[idx][0]),self.data[idx][2]\n",
    "        single_audio_=torch.nn.functional.pad(single_audio,(0,tensor_max_length-single_audio.shape[2]),mode='constant',value=0)\n",
    "        single_label=torch.tensor(list(map(alpha_to_num,transcript)),dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "                'data':single_audio_.transpose(-1,-2),\n",
    "                'label':torch.nn.functional.pad(single_label,(0,label_max_length-single_label.shape[-1]),mode='constant',value=0),\n",
    "                'data_len': torch.tensor(single_audio.shape[2],dtype=torch.long),\n",
    "                'label_len':torch.tensor(len(transcript),dtype=torch.long)\n",
    "                }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "train_loader=DataLoader(Dataloader(train_data,transforms=train_transforms),batch_size=Config.train_batch_size,shuffle=True)\n",
    "Val_Loader=DataLoader(Dataloader(val_data,transforms=val_transforms),batch_size=Config.val_batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layernorm(x,input_features):\n",
    "    Layer=nn.LayerNorm(input_features)\n",
    "    Layer.to(Config.device)\n",
    "    return Layer(x)\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel,stride,padding,dropout_probability,input_features):\n",
    "        super().__init__()\n",
    "        self.input_features=input_features\n",
    "        self.conv_1=nn.Conv2d(in_channels,out_channels,kernel,stride,padding)\n",
    "        self.conv_2=nn.Conv2d(out_channels,out_channels,kernel,stride,padding)  \n",
    "        self.drop_1=nn.Dropout(dropout_probability)\n",
    "        self.drop_2=nn.Dropout(dropout_probability)\n",
    "    \n",
    "    def forward(self,batch):\n",
    "        residue=batch\n",
    "        batch=f.gelu(layernorm(batch,self.input_features))\n",
    "        batch=self.drop_1(batch)\n",
    "        batch=self.conv_1(batch)\n",
    "        batch=f.gelu(layernorm(batch,self.input_features))\n",
    "        batch=self.drop_2(batch)\n",
    "        batch=self.conv_2(batch)\n",
    "        return batch+residue\n",
    "    \n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers,dropout_probability):\n",
    "        \"\"\"\n",
    "        input_size=n_features\n",
    "        hidden_size= nom feature in the hidden state\n",
    "        num_layers= how many rnns do you wanna stack\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.bi_gru=nn.GRU(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True,bidirectional=True)\n",
    "        self.drop=nn.Dropout(dropout_probability)\n",
    "        \n",
    "    def forward(self,batch):\n",
    "        batch=f.gelu(layernorm(batch,self.input_size))\n",
    "        batch=self.bi_gru(batch)\n",
    "        batch=self.drop(batch[0] if type(batch)==tuple else batch)\n",
    "        return batch\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# model inspired from deep speech 2\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn=nn.Conv2d(in_channels=1,\n",
    "                            out_channels=32,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1)\n",
    "        \n",
    "        self.linear=nn.Linear(Config.res_cnn_output_channels*Config.input_features,Config.bi_rnn_input_size) \n",
    "        \n",
    "        self.res_cnn=nn.Sequential(*[ResidualCNN(in_channels=Config.res_cnn_input_channels,\n",
    "                                                 out_channels=Config.res_cnn_output_channels,\n",
    "                                                 kernel=Config.res_cnn_kernel_size,\n",
    "                                                 stride=Config.res_cnn_stride,\n",
    "                                                 padding=Config.res_cnn_padding,\n",
    "                                                 dropout_probability=Config.res_cnn_dropout_probability,\n",
    "                                                 input_features=Config.input_features) for _ in range(Config.num_res_cnn)])\n",
    "        \n",
    "        self.rnn_layers=nn.Sequential(*[BidirectionalGRU(input_size=Config.bi_rnn_input_size if i==0 else 2*Config.bi_rnn_input_size,\n",
    "                                                         hidden_size=Config.bi_rnn_hidden_size,\n",
    "                                                         num_layers=Config.bi_rnn_num_layers,\n",
    "                                                         dropout_probability=Config.bi_rnn_dropout_probability) for i in range(Config.num_rnn)])\n",
    "        self.classifier=nn.Sequential(\n",
    "                                    nn.Linear(Config.cl_input_size,Config.cl_input_size//2),\n",
    "                                    nn.GELU(),\n",
    "                                    nn.Dropout(Config.cl_dropout_probability),\n",
    "                                    nn.Linear(Config.cl_input_size//2,Config.cl_num_classes))\n",
    "        \n",
    "    def forward(self,batch):\n",
    "        time_dim=batch.shape[2]\n",
    "        batch_size=batch.shape[0]\n",
    "        out=self.cnn(batch)\n",
    "        out=self.res_cnn(out)\n",
    "        out=out.view(batch_size,time_dim,Config.res_cnn_output_channels*Config.input_features) \n",
    "        out=self.linear(out)\n",
    "        out=self.rnn_layers(out)\n",
    "        out=self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SpeechRecognitionModel()\n",
    "model=model.to(Config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define \n",
    "* optimizer\n",
    "* scheduler\n",
    "* criterion or cost function or Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.AdamW(model.parameters(),lr=Config.lr)\n",
    "scheduler=optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=Config.lr,\n",
    "    steps_per_epoch=int(len(train_loader)),\n",
    "    epochs=Config.epochs,\n",
    "    anneal_strategy='linear')\n",
    "\n",
    "criterion=nn.CTCLoss(blank=28).to(Config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, optimizer, scheduler, model, criterion):\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.model = model.to(Config.device)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def fit(self, train_loader):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        for idx, batch in tqdm(enumerate(train_loader), desc='Training', total=len(train_loader)):\n",
    "            X_train = batch['data'].to(Config.device)\n",
    "            y_train = batch['label'].to(Config.device)\n",
    "            X_train_len = batch['data_len'].to(Config.device)\n",
    "            y_train_len = batch['label_len'].to(Config.device)\n",
    "\n",
    "            out = self.model(X_train)  # [B, T, C]\n",
    "            out = F.log_softmax(out, dim=2)  # softmax over class dimension\n",
    "            out = out.permute(1, 0, 2)       # [T, B, C] for CTC loss\n",
    "\n",
    "            loss = self.criterion(out, y_train, X_train_len, y_train_len)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        return np.mean(losses)\n",
    "\n",
    "    def predict(self, val_loader):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in tqdm(enumerate(val_loader), desc='Validating', total=len(val_loader)):\n",
    "                X_val = batch['data'].to(Config.device)\n",
    "                y_val = batch['label'].to(Config.device)\n",
    "                X_val_len = batch['data_len'].to(Config.device)\n",
    "                y_val_len = batch['label_len'].to(Config.device)\n",
    "\n",
    "                out = self.model(X_val)               # [B, T, C]\n",
    "                out = F.log_softmax(out, dim=2)\n",
    "                out = out.permute(1, 0, 2)            # [T, B, C]\n",
    "\n",
    "                loss = self.criterion(out, y_val, X_val_len, y_val_len)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Train(optimizer, scheduler, model, criterion)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in tqdm(range(Config.epochs), desc='Epochs'):\n",
    "    train_loss = train.fit(train_loader)\n",
    "    val_loss = train.predict(val_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{Config.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(val_losses)),val_losses)\n",
    "plt.plot(range(len(train_losses)),train_losses)\n",
    "plt.legend(['vl','tr'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read\n",
    "- [CTCLoss](https://distill.pub/2017/ctc/?undefined)\n",
    "- [Mel-Frequency](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html?undefined)\n",
    "- [LibriSpeech](https://www.openslr.org/12/?undefined=&ref=news-tutorials-ai-research)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
