{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d6dgGoLzDwEv","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as scheduler\n","import transformers\n","import pandas as pd\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import f1_score\n","import numpy as np\n","from tqdm.autonotebook import tqdm\n","from sklearn.model_selection import StratifiedKFold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wpXxdy6BDwE2","trusted":true},"outputs":[],"source":["class Config:\n","    device='cuda'if torch.cuda.is_available() else 'cpu'\n","    train_batch_size=8\n","    val_batch_size=8\n","    seed=5\n","    n_splits=5\n","    pooler_output_dim=768\n","    metric=f1_score\n","    lr=1e-4\n","    step_size=10\n","    gamma=0.1\n","    epochs=5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8lA_qYpyDwE3","trusted":true},"outputs":[],"source":["checkpoint=\"bert-base-cased\"\n","bert=transformers.AutoModel.from_pretrained(checkpoint)\n","bert_tokenizer=transformers.AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnJ-Oe9oDwE4","trusted":true},"outputs":[],"source":["data=pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n","data['sentiment']=list(map(lambda x : 1 if x==\"positive\" else 0,data['sentiment'].values))\n","\n","\n","from sklearn.model_selection import StratifiedKFold\n","data.loc[:,'kfold']=-1\n","data.sample(frac=1,random_state=Config.seed).reset_index(drop=True)\n","labels=data['sentiment'].values\n","skf=StratifiedKFold(n_splits=Config.n_splits)\n","\n","for fold,(train,val) in enumerate(skf.split(X=data,y=data['sentiment'])):\n","    data.loc[val,'kfold']=fold\n","data.to_csv('/kaggle/working/folds.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Cg-S_a6DwE5","trusted":true},"outputs":[],"source":["class ReviewData:\n","    def __init__(self,data):\n","        self.data=data.reset_index(drop=True)\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self,idx):\n","        review=bert_tokenizer(self.data['review'][idx],truncation=True,max_length=300,padding='max_length',return_tensors='pt')\n","        sentiment=torch.tensor(self.data['sentiment'][idx],dtype=torch.long)\n","        return {'x':{k:v.to(device=Config.device) for k,v in review.items()},'y':sentiment.to(device=Config.device)}\n","\n","    \n","    \n","class Classif_Model(nn.Module):\n","    def __init__(self,bert_model,hidden_shape,out_shape):\n","        super().__init__()\n","        self.bert_model=bert_model\n","        self.hidden_shape=hidden_shape\n","        self.out_shape=out_shape\n","        self.layers=nn.Sequential(\n","            nn.Linear(Config.pooler_output_dim,hidden_shape),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.1),\n","            nn.Linear(hidden_shape,out_shape)\n","        \n","        )\n","    \n","    def forward(self,batch):\n","        out=self.bert_model(batch)\n","        out=out['pooler_output']\n","        out=self.layers(out)\n","        return out\n","    \n","model=Classif_Model(bert,100,2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8iR7E5zDwE7","trusted":true},"outputs":[],"source":["class Train:\n","    def __init__(self,model,criterion,optimizer,scheduler):\n","        self.model=model\n","        self.criterion=criterion\n","        self.optimizer=optimizer\n","        self.scheduler=scheduler\n","    \n","    @staticmethod\n","    def logits_to_prediction(output):\n","        preds=torch.argmax(torch.softmax(output.detach().to('cpu'),dim=-1),dim=-1)\n","        return preds.view(1,-1)\n","    @staticmethod\n","    def return_metric(preds,labels):\n","        return Config.metric(preds.to('cpu'),labels.to('cpu'),average='weighted')\n","    \n","    def fit(self,data_loader):\n","        self.model.train()\n","        for batch in data_loader:\n","            out=self.model(batch['x']['input_ids'].squeeze_(1)) \n","            loss=self.criterion(out,batch['y'])\n","            loss.backward()\n","            self.optimizer.zero_grad()\n","            self.optimizer.step()\n","            self.scheduler.step()\n","            \n","    def predict_metric(self,data_loader,type_='train'):\n","        batch_loss=0\n","        if type_=='train':\n","            batch_size=Config.train_batch_size\n","        elif type_=='val':\n","            batch_size=Config.val_batch_size\n","            \n","        preds=torch.empty([len(data_loader),batch_size])\n","        labels=torch.empty([len(data_loader),batch_size])\n","        \n","        with torch.no_grad():\n","            self.model.eval()\n","            for idx,batch in enumerate(data_loader):\n","                out=self.model(batch['x']['input_ids'].squeeze_(1)) \n","                batch_loss+=self.criterion(out,batch['y']).item()\n","                preds[idx,:]=self.logits_to_prediction(out)\n","                labels[idx,:]=batch['y'].to('cpu')\n","        \n","            metric=self.return_metric(preds,labels)\n","        return metric,batch_loss/len(data_loader)\n","    \n","             \n","            \n","folds=pd.read_csv(\"/kaggle/working/folds.csv\")\n","model=model.to(device=Config.device)\n","criterion=nn.CrossEntropyLoss()\n","optimizer=optim.Adam(model.parameters(),lr=Config.lr)\n","scheduler=optim.lr_scheduler.StepLR(optimizer,step_size=Config.step_size,gamma=Config.gamma)\n","train=Train(model,criterion,optimizer,scheduler)\n","\n","epoch_train_loss=[] #per epoch\n","epoch_val_loss=[]   #per epoch\n","epoch_train_f1=[]   #per epoch\n","epoch_val_f1=[]     #per epoch\n","\n","for epoch in tqdm(range(Config.epochs)):\n","\n","    train_loss=[] \n","    val_loss=[]   \n","    train_f1=[]   \n","    val_f1=[]  \n","    \n","    for fold in range(Config.n_splits):\n","        \n","        train_data=DataLoader(ReviewData(folds.loc[folds['kfold']!=fold]),batch_size=Config.train_batch_size,drop_last=True)\n","        val_data=DataLoader(ReviewData(folds.loc[folds['kfold']==fold]),batch_size=Config.val_batch_size,drop_last=True)\n","        train.fit(train_data)\n","        \n","        t_f1,t_loss=train.predict_metric(train_data,'train')\n","        train_loss.append(t_loss)\n","        train_f1.append(t_f1)\n","        \n","        \n","        v_f1,v_loss=train.predict_metric(val_data,'val')\n","        val_loss.append(v_loss)\n","        val_f1.append(v_f1)\n","        \n","    epoch_train_loss.append(np.mean(train_loss))\n","    epoch_train_f1.append(np.mean(train_f1))\n","    epoch_val_loss.append(np.mean(val_loss))\n","    epoch_val_f1.append(np.mean(val_f1))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQ3eDo91DwE9","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.plot(range(Config.epochs),epoch_train_loss)\n","plt.plot(range(Config.epochs),epoch_val_loss)\n","plt.plot(range(Config.epochs),epoch_train_f1)\n","plt.plot(range(Config.epochs),epoch_val_f1)\n","plt.legend(['train_loss','val_loss','train_f1','val_f1'],bbox_to_anchor=(1.05,1),loc='upper left');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mdS9YwuDwE-"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}
